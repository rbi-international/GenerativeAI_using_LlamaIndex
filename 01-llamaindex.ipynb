{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b8964-2ceb-4aa7-aa50-8f2934c017c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script loads environment variables from a .env file and retrieves your OpenAI API key \n",
    "securely from your system's environment variables.\n",
    "\n",
    "Requirements:\n",
    "- python-dotenv package\n",
    "- A .env file in your project root containing: OPENAI_API_KEY=<your_api_key>\n",
    "\n",
    "Example .env:\n",
    "    OPENAI_API_KEY=\"sk-xxxxxxx\"\n",
    "\n",
    "Usage:\n",
    "    python your_script.py\n",
    "\"\"\"\n",
    "\n",
    "# Import the built-in 'os' module for interacting with the operating system (e.g., environment variables)\n",
    "import os\n",
    "\n",
    "# Import 'load_dotenv' to load variables from a .env file,\n",
    "# and 'find_dotenv' to locate the .env file automatically.\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Locate the .env file and load its contents so that variables are available as environment variables.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve the OpenAI API key from the environment variables.\n",
    "# This avoids hardcoding sensitive keys in your code.\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faf7ee-579e-45e2-b22c-4c5ebf7e2d98",
   "metadata": {},
   "source": [
    "## Load private document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9cd4d-173b-4f4a-a992-8a770c741d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script uses LlamaIndex (formerly GPT Index) to:\n",
    "\n",
    "1Ô∏è‚É£ Import key classes for building and loading vector indexes.\n",
    "2Ô∏è‚É£ Read all documents from a local 'data' folder.\n",
    "\n",
    "Requirements:\n",
    "- llama-index==0.12.39\n",
    "- A 'data/' directory in your project root containing text files or documents to index.\n",
    "\n",
    "Usage:\n",
    "    python your_script.py\n",
    "\"\"\"\n",
    "\n",
    "# Import core LlamaIndex components:\n",
    "# - VectorStoreIndex: for creating a vector-based index of documents.\n",
    "# - SimpleDirectoryReader: for reading files from a folder.\n",
    "# - StorageContext and load_index_from_storage: for saving/loading indexes efficiently.\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# Create a SimpleDirectoryReader instance pointing to the 'data' folder.\n",
    "# It automatically finds and reads text files (e.g., .txt, .md, .pdf if supported).\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0238d1-3bcf-4ded-a769-98d8d93729ef",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63975112-0f22-4edd-8c7b-1b4406f8e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This line builds a vector-based index from the loaded documents using LlamaIndex.\n",
    "\n",
    "What happens here?\n",
    "üìö -> üßÆ -> ‚ö°Ô∏è\n",
    "\n",
    "1Ô∏è‚É£ Takes your raw documents.\n",
    "2Ô∏è‚É£ Converts them into numerical embeddings (vectors).\n",
    "3Ô∏è‚É£ Stores them in a vector index, making them easy for LLMs to search and retrieve relevant chunks.\n",
    "\n",
    "Usage:\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\"\"\"\n",
    "\n",
    "# Create a new vector index from the list of documents.\n",
    "# Under the hood:\n",
    "# - It chunks large documents if needed.\n",
    "# - Embeds each chunk using the chosen embedding model.\n",
    "# - Stores the embeddings in memory (or later on disk).\n",
    "index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159feeb-7eb4-4911-bbf1-714de67dfe69",
   "metadata": {},
   "source": [
    "## Ask questions to private document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede51e4-1aec-4a70-9007-7a15b4e6a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the importance of creating something people want and not focusing solely on making money in the early stages of a startup. It explores the idea of startups behaving like nonprofits and how benevolence can benefit startups in various ways, such as improving morale, attracting help from others, and aiding decision-making. Examples like Craigslist and Google are used to illustrate how successful companies have incorporated elements of charity. The article emphasizes the significance of mission-driven work, user care, and the tamagotchi effect in sustaining startups through challenges and fostering investor interest.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block turns the vector index into an interactive query engine\n",
    "and sends it a question: \"Summarize the article in 100 words.\"\n",
    "\n",
    "Steps:\n",
    "1Ô∏è‚É£ Converts the index into a query engine capable of semantic search + generation.\n",
    "2Ô∏è‚É£ Runs your prompt against the engine.\n",
    "3Ô∏è‚É£ Prints the LLM-powered answer.\n",
    "\n",
    "Usage:\n",
    "    python your_script.py\n",
    "\"\"\"\n",
    "\n",
    "# Create a query engine from the index.\n",
    "# This engine knows how to:\n",
    "# - Look up relevant text chunks via embeddings.\n",
    "# - Feed those chunks to your LLM (e.g., OpenAI).\n",
    "# - Return a coherent answer.\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Ask the query engine to summarize the whole article in 100 words.\n",
    "response = query_engine.query(\"summarize the article in 100 words\")\n",
    "\n",
    "# Print the response from the LLM.\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c19db6-15c5-497a-b018-9c3a3cf12511",
   "metadata": {},
   "source": [
    "## See under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fa5e0-ee57-48fa-a343-c9662065709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This (optional) logging setup helps you see detailed debug info from LlamaIndex \n",
    "and other libraries in your console.\n",
    "\n",
    "When enabled, it:\n",
    "1Ô∏è‚É£ Sends all logs to your console (stdout).\n",
    "2Ô∏è‚É£ Sets the logging level to DEBUG ‚Äî so you see everything, even low-level messages.\n",
    "3Ô∏è‚É£ Adds an explicit StreamHandler to make sure logs appear nicely.\n",
    "\n",
    "Usage:\n",
    "    Uncomment this block for debugging or development.\n",
    "\"\"\"\n",
    "\n",
    "# Import the built-in logging module to handle logs.\n",
    "# import logging\n",
    "\n",
    "# Import sys so you can use sys.stdout (the console output stream).\n",
    "# import sys\n",
    "\n",
    "# Configure logging:\n",
    "# - stream=sys.stdout: print logs to your terminal, not to a file.\n",
    "# - level=logging.DEBUG: show all messages, even the most verbose.\n",
    "# logging.basicConfig(\n",
    "#     stream=sys.stdout, \n",
    "#     level=logging.DEBUG\n",
    "# )\n",
    "\n",
    "# Add an extra StreamHandler so you can see logs in real time.\n",
    "# Useful if other handlers suppress them.\n",
    "# logging.getLogger().addHandler(\n",
    "#     logging.StreamHandler(stream=sys.stdout)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c62d2e-4ebc-473a-b215-8d719e87fe3e",
   "metadata": {},
   "source": [
    "## Save the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0043ba-f669-4c0f-b67f-1415da83c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the author, being good is not about claiming to be a particularly good person or having sanctimonious intentions. Instead, being good is suggested because it works effectively as a guide to strategy, a design spec for software, and a valuable approach for startups.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script does smart index handling for a Generative AI pipeline using LlamaIndex:\n",
    "\n",
    "‚úÖ Checks if a local vector index already exists in './storage'.\n",
    "    - If it DOES NOT exist:\n",
    "        1Ô∏è‚É£ Reads documents from the 'data' folder.\n",
    "        2Ô∏è‚É£ Creates a fresh vector index.\n",
    "        3Ô∏è‚É£ Saves (persists) the index to './storage' for reuse.\n",
    "    - If it DOES exist:\n",
    "        1Ô∏è‚É£ Loads the prebuilt index from './storage'.\n",
    "\n",
    "‚úÖ Then, it turns the index into a query engine.\n",
    "\n",
    "‚úÖ Finally, it queries: \"According to the author, what is good?\" \n",
    "   and prints the LLM-powered answer.\n",
    "\n",
    "Usage:\n",
    "    python your_script.py\n",
    "\n",
    "Requirements:\n",
    "    - llama-index==0.12.39\n",
    "    - data/ folder with documents\n",
    "    - storage/ folder is created automatically on first run\n",
    "\"\"\"\n",
    "\n",
    "# Import the 'os.path' module to work with filesystem paths.\n",
    "import os.path\n",
    "\n",
    "# Check if a persistent index already exists in the 'storage' directory.\n",
    "if not os.path.exists(\"./storage\"):\n",
    "    # If storage doesn't exist:\n",
    "    # 1. Read all documents from the 'data' folder.\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    \n",
    "    # 2. Create a new vector index from the documents.\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    \n",
    "    # 3. Persist (save) the index and its embeddings to disk for next time.\n",
    "    index.storage_context.persist()\n",
    "else:\n",
    "    # If storage exists:\n",
    "    # 1. Create a StorageContext using the saved folder.\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    \n",
    "    # 2. Load the existing index from disk.\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Now, whether we built it or loaded it, we have an index ready for querying.\n",
    "\n",
    "# Create a query engine from the index.\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Run a semantic query: \"According to the author, what is good?\"\n",
    "response = query_engine.query(\"According to the author, what is good?\")\n",
    "\n",
    "# Print the LLM's answer.\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c257e6-0fac-4a32-b1ac-b78338d3a929",
   "metadata": {},
   "source": [
    "## Customization options\n",
    "* parse into smaller chunks\n",
    "* use a different vector store\n",
    "* retrieve more context when I query\n",
    "* use a different LLM\n",
    "* use a different response mode\n",
    "* stream the response back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f150c-7df3-48f9-98fa-b8bd55f696c8",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "* QA\n",
    "* Chatbot\n",
    "* Agent\n",
    "* Structured Data Extraction\n",
    "* Multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf6e50-31b8-45fe-af63-311876d79641",
   "metadata": {},
   "source": [
    "## Optimizing\n",
    "* Advanced Retrieval Strategies\n",
    "* Evaluation\n",
    "* Building performant RAG applications for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8a462-4952-48cc-84cb-864841cecbc8",
   "metadata": {},
   "source": [
    "## Other\n",
    "* LlamaPacks and Create-llama = LangChain templates\n",
    "* Very recent, still in beta\n",
    "* Very interesting: create-llama allows you to create a Vercel app!\n",
    "* Very interesting: open-source end-to-end project (SEC Insights)\n",
    "    * llamaindex + react/nextjs (vercel) + fastAPI + render + AWS\n",
    "    * environment setup: localStack + docker\n",
    "    * monitoring: sentry\n",
    "    * load testing: loader.io\n",
    "    * [web](https://www.secinsights.ai/)\n",
    "    * [code](https://github.com/run-llama/sec-insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59ffe0-ded8-41ef-9c1b-3d4af53215bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
