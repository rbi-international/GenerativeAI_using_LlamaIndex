# ğŸ“š Generative AI with LlamaIndex, Chroma DB & PaLM

### How to run?
```bash
1. conda create -n llama python=3.13.2 -y

2. conda activate llama

3. pip install -r requirements.txt
```


This project is inspired by [Data Science with Bappyâ€™s Udemy course](https://www.udemy.com/course/generative-ai-with-ai-agents-mcp-for-developers/learn/lecture/50262741#overview).
It shows how to build a local knowledge base using **LlamaIndex**, store embeddings efficiently with **ChromaDB**, and query them using an LLM like **OpenAI GPT** or **PaLM**.

## ğŸš€ Features

âœ… Load documents from a local `data/` folder  
âœ… Create a vector index (`VectorStoreIndex`)  
âœ… Store embeddings in **ChromaDB**  
âœ… Configure chunk size & LLM using **Settings**  
âœ… Reuse saved indexes  
âœ… Use OpenAI or Google PaLM as LLM  
âœ… Query your knowledge base naturally  

## ğŸ—‚ï¸ Project Structure

```
GenerativeAI_using_LlamaIndex/
â”œâ”€â”€ data/                  # Your documents
â”œâ”€â”€ chroma_db/             # ChromaDB storage
â”œâ”€â”€ storage/               # LlamaIndex index storage
â”œâ”€â”€ 01-llamaindex.ipynb    # Core notebook
â”œâ”€â”€ 02-llamaindex-deep-dive.ipynb  # Advanced notebook
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

## âš™ï¸ Quick Start

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
pip install llama-index-vector-stores-chroma
pip install llama-index-llms-palm  # Optional, for PaLM
```

### 2ï¸âƒ£ Prepare `.env`

```env
OPENAI_API_KEY="your_openai_api_key"
# For PaLM, set up Google Cloud credentials:
# export GOOGLE_APPLICATION_CREDENTIALS="path/to/your-service-account.json"
```

### 3ï¸âƒ£ Run notebooks

Open:
- `01-llamaindex.ipynb`
- `02-llamaindex-deep-dive.ipynb`

## ğŸ”‘ Key Code

```python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# Load data
documents = SimpleDirectoryReader("data").load_data()

# Setup Chroma
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("quickstart")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Create index
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
index.storage_context.persist()

# Query
query_engine = index.as_query_engine(similarity_top_k=5)
response = query_engine.query("In less than 100 words, what is the meaning of good for the author?")
print(response)
```

## ğŸ“Œ Reference

Based on the [Generative AI with AI Agents (MCP) for Developers](https://www.udemy.com/course/generative-ai-with-ai-agents-mcp-for-developers/learn/lecture/50262741#overview) course by **Data Science with Bappy**, extended with latest **LlamaIndex** best practices.

## âœ… Requirements

```txt
llama-index==0.12.39
llama-index-vector-stores-chroma
llama-index-llms-palm  # optional
chromadb==1.0.12
python-dotenv==1.1.0
```

## ğŸš€ Happy building!

